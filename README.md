# ðŸ•·ï¸ SAS.ScrapingAgent

**SAS.ScrapingAgent** is a scalable, modular scraping service that collects and processes real-time data from platforms like **Telegram**, **Nitter**, **Twitter (API)**, and others. It is part of a larger microservices-based system designed for monitoring and detecting local events such as crimes and disasters using social media content.

---

## ðŸ“¦ Features

* ðŸ”Œ Supports multiple scraping sources (Telegram, Nitter, Twitter, etc.)
* ðŸ“„ Extracts structured data
* ðŸŽ¯ Pipeline for preprocessing and filtering
* âš™ï¸ Easily extendable with new scraper modules
* ðŸ›  Uses **Selenium**, **Playwright**, or **API** for scraping
* ðŸ” Asynchronous support for high throughput
* ðŸ”„ Integrates with Kafka for event publishing

---

## ðŸ§± Project Structure

```
src/
â””â”€â”€ app/
    â”œâ”€â”€ core/                  # Domain layer
    â”œâ”€â”€ pipeline/              # Data processing pipeline
    â”œâ”€â”€ scrapers/              # Scraper modules (Telegram, Twitter, etc.)
    â”‚   â””â”€â”€ sources/
    â”‚       â”œâ”€â”€ telegram/
    â”‚       â”œâ”€â”€ twitter/       # âœ… NEW: Twitter API scraper
    â”‚       â”œâ”€â”€ nitter/
    â”‚       â””â”€â”€ dummy/
    â”œâ”€â”€ kafka/                 # Kafka consumer/producer integrations
    â””â”€â”€ main.py                # Entry point
tests/                         # Unit & performance tests
```

---

## ðŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/hasankhadd0ur/sas.scrapingagent.git
cd sas.scrapingagent
```

### 2. Create a virtual environment

```bash
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ðŸ§ª Usage

### Run scrapers manually

```bash
python -m app.main
```

The main agent will:

* Load active scraping tasks
* Run platform-specific scrapers (Telegram, Twitter, etc.)
* Filter and transform collected messages
* Optionally publish them to Kafka or store locally

---

## âœï¸ Adding a New Scraper

1. Create a module under `app/scrapers/sources/`
2. Inherit from `BaseScraper`
3. Implement:

```python
async def run_task(self, task: ScrapingTask) -> AsyncGenerator[List[Message], None]
```

Example:

```python
class MyScraper(BaseScraper):
    async def run_task(self, task):
        # your scraping logic
        yield [Message(...), ...]
```

Then register your scraper in `scrapers_registry.py`.

---

## ðŸ“¤ Kafka Integration (optional)

The system can forward messages to Kafka. See:

* `kafka_producer.py`
* `messages_publishing_stage.py`

To enable:

1. Add Kafka configuration to your config service.
2. Include `MessagesPublishingStage` in your pipeline.

---

## ðŸ§ª Running Tests

```bash
pytest tests/
```

---

## ðŸ§© Supported Scrapers

| Source       | Method   | File                                                      |
| ------------ | -------- | --------------------------------------------------------- |
| Telegram     | API/Web  | `telegram_telethon_scraper.py`, `telegram_web_scraper.py` |
| Twitter      | API (v2) | `twitter_api_scraper.py`                                  |
| Nitter       | Web      | `nitter_web_scraper.py`                                   |
| Dummy Source | File     | `dummy_file_scrarper.py`                                  |

---

## ðŸ“„ License

MIT License

