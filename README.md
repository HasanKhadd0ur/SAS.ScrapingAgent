# ğŸ•·ï¸ SAS.ScrapingAgent

**SAS.ScrapingAgent** is a scalable, modular scraping service that collects and processes real-time data from platforms like **Telegram**, **Nitter**, and others. It is part of a larger microservices-based system designed for monitoring and detecting local events such as crimes and disasters using social media content.

---

## ğŸ“¦ Features

* ğŸ”Œ Supports multiple scraping sources (Telegram, Nitter, etc.)
* ğŸ“„ Extracts structured data
* ğŸ¯ Pipeline for preprocessing and filtering
* âš™ï¸ Easily extendable with new scraper modules
* ğŸ›  Uses **Selenium** or **Playwright** for web scraping
* ğŸ” Asynchronous support for high throughput
* ğŸ”„ Integrates with Kafka for event publishing

---

## ğŸ§± Project Structure

```
src/
â””â”€â”€ app/
    â”œâ”€â”€ core/                  # Domain layer
    â”‚   â”œâ”€â”€ logging/           # Logging setup (optional/custom)
    â”‚   â”œâ”€â”€ models/            # Core domain models (e.g. Message, ScraperTask)
    â”‚   â””â”€â”€ services/          # Reusable logic or integrations (e.g. Kafka producer)
    â”‚
    â”œâ”€â”€ pipeline/              # Data processing pipeline
    â”‚   â”œâ”€â”€ stages/            # Individual pipeline steps (e.g. keyword filtering)
    â”‚   â”œâ”€â”€ base.py            # Pipeline base interface
    â”‚   â”œâ”€â”€ registry.py        # Dynamic registration of stages
    â”‚   â””â”€â”€ pipeline.py        # Main pipeline implementation
    â”‚
    â”œâ”€â”€ scrapers/              # Data source modules
    â”‚   â”œâ”€â”€ base.py            # Abstract scraper interface
    â”‚   â”œâ”€â”€ telegram/          # Telegram scraper implementation
    â”‚   â””â”€â”€ nitter/            # Nitter (Twitter frontend) scraper implementation
    â”‚
    â”œâ”€â”€ main.py                # Application entry point
    â””â”€â”€ __init__.py
tests/                         # Unit and integration tests

```

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/your-user/sas.scrapingagent.git
cd sas.scrapingagent
```

### 2. Create a virtual environment

```bash
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment

Edit `config.py` or use `.env` for:

```env
API_ID=your_telegram_api_id
API_HASH=your_telegram_api_hash
SESSION=session_name
DESTINATION=destination_channel_or_user
CHATS=source_channel_ids_or_usernames
KEY_WORDS=keywords,to,filter,by
```

---

## ğŸ§ª Usage

### Run a specific scraper manually

```bash
python -m app.main
```

The main agent will:

* Run enabled scraper tasks
* Fetch, filter, and transform messages
* Optionally send results to a Kafka topic or save locally

---

## âœï¸ Adding a New Scraper

1. Create a new folder under `app/scrapers/`
2. Inherit from `BaseScraper`
3. Implement `run_task(self, task: ScraperTask) -> List[Message]`

Example:

```python
class YourCustomScraper(BaseScraper):
    def run_task(self, task: ScraperTask) -> List[Message]:
        ...
```

---

## ğŸ§ª Running Tests

```bash
pytest tests/
```

---

## ğŸ“¤ Kafka Integration (optional)

This project can forward results to Kafka topics by implementing a `KafkaProducer` and integrating it into `main.py`.

---

## ğŸ“„ License

MIT License
